import cv2
from ultralytics import YOLO  # ä»…å¯¼å…¥YOLOï¼Œè·Ÿè¸ªå†…ç½®
import time
import numpy as np
import logging
import json
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import threading
from queue import Queue

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('monitor.log')
    ]
)
logger = logging.getLogger(__name__)

# RTSP URL
RTSP_URL = "rtsp://admin:Aa147258@192.168.109.213"

# ä¿å­˜ç›®å½•
BASE_DIR = Path("detection_results")
BASE_DIR.mkdir(exist_ok=True)

IMAGES_DIR = BASE_DIR / "images"
EVENTS_DIR = BASE_DIR / "events"
DETECTIONS_DIR = BASE_DIR / "detections"

for directory in [IMAGES_DIR, EVENTS_DIR, DETECTIONS_DIR]:
    directory.mkdir(exist_ok=True)

# åŠ è½½YOLOæ¨¡å‹ï¼ˆå†…ç½®BOTSORTè·Ÿè¸ªï¼‰
model = YOLO('yolov8m.pt')

# æ£€æµ‹å‚æ•°
person_class = 0
vehicle_classes = [2, 3, 5, 7]
conf_threshold = 0.5
classes_to_detect = [person_class] + vehicle_classes

# ä¿å­˜é…ç½®
SAVE_IMAGES = True
SAVE_EVENTS = True
EVENT_DURATION = 10  # ç§’
PERSON_DEDUPE_FRAMES = 30  # å»é‡é—´éš”

# è·Ÿè¸ªè®°å½•
track_history = {}
stats = defaultdict(int)
total_detections = 0
unique_tracks = set()

# äº‹ä»¶å½•åˆ¶
event_writer = None
event_start_time = None
save_queue = Queue(maxsize=50)

def get_timestamp():
    return datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]

def classify_object(cls_id):
    """å¯¹è±¡åˆ†ç±»"""
    if cls_id == person_class:
        return "person"
    elif cls_id in vehicle_classes:
        return "vehicle"
    return "other"

def analyze_behavior(bbox, frame_shape):
    """è¡Œä¸ºåˆ†æ"""
    x1, y1, x2, y2 = bbox
    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
    h, w = frame_shape[:2]
    
    behavior = "center"
    if center_x < w * 0.3: behavior = "left"
    elif center_x > w * 0.7: behavior = "right"
    if center_y < h * 0.3: behavior = behavior + "_top"
    elif center_y > h * 0.7: behavior = behavior + "_bottom"
    
    return behavior

def should_save_person(track_id, current_frame):
    """äººå‘˜å»é‡"""
    if track_id not in track_history:
        return True
    last_saved = track_history[track_id].get("last_frame", 0)
    return current_frame - last_saved >= PERSON_DEDUPE_FRAMES

def console_output(detections, frame_id):
    """æ§åˆ¶å°è¾“å‡º"""
    global total_detections

    if not detections:
        return

    unique_ids = {det['track_id'] for det in detections}
    print("\n" + "="*80)
    print(f"ğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | å¸§:{frame_id} | å”¯ä¸€ID:{len(unique_ids)}")

    persons = [d for d in detections if d['type'] == 'person']
    vehicles = [d for d in detections if d['type'] == 'vehicle']

    print(f"ğŸ‘¤ äººå‘˜:{len(persons)} | ğŸš— è½¦è¾†:{len(vehicles)}")

    for det in detections:
        emoji = "ğŸ‘¤" if det['type'] == 'person' else "ğŸš—"
        print(f"  {emoji} ID:{det['track_id']} | {det['type']} | {det['behavior']} | {det['confidence']:.2f}")

    stats['person'] += len(persons)
    stats['vehicle'] += len(vehicles)
    total_detections += len(detections)
    print(f"ğŸ“Š ç´¯è®¡: äºº={stats['person']}, è½¦={stats['vehicle']}, æ€»è®¡={total_detections}")
    print("="*80)

def save_detection_image(frame, detections, frame_id):
    """ä¿å­˜å»é‡å›¾åƒ"""
    # åªä¿å­˜ç¬¦åˆæ¡ä»¶çš„äººå‘˜
    persons_to_save = [
        det for det in detections 
        if det['type'] == 'person' and should_save_person(det['track_id'], frame_id)
    ]
    
    if not persons_to_save:
        return None
    
    # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹æ¡†
    for det in detections:
        x1, y1, x2, y2 = det['bbox']
        color = (0, 255, 0) if det['type'] == 'person' else (255, 0, 0)
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        label = f"ID:{det['track_id']} {det['confidence']:.1f}"
        cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    # å›¾åƒä¿¡æ¯
    timestamp = get_timestamp()
    filename = f"person_{frame_id}_{timestamp}.jpg"
    filepath = IMAGES_DIR / filename
    
    summary = f"å¸§:{frame_id} | äººå‘˜:{len(persons_to_save)}"
    cv2.putText(frame, summary, (10, frame.shape[0]-20), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    cv2.imwrite(str(filepath), frame)
    logger.info(f"ğŸ’¾ ä¿å­˜å›¾åƒ: {filepath}")
    
    # æ›´æ–°å†å²
    for person in persons_to_save:
        track_history[person['track_id']] = {
            "last_frame": frame_id,
            "save_count": track_history.get(person['track_id'], {}).get("save_count", 0) + 1
        }
    
    unique_tracks.update([p['track_id'] for p in persons_to_save])
    return str(filepath)

def save_detection_data(detections, image_path, frame_id):
    """ä¿å­˜JSONæ•°æ®"""
    record = {
        "timestamp": datetime.now().isoformat(),
        "frame_id": frame_id,
        "image_path": image_path,
        "detections": detections,
        "statistics": dict(stats)
    }
    
    timestamp = get_timestamp()
    filename = f"detection_{frame_id}_{timestamp}.json"
    filepath = DETECTIONS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(record, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info(f"ğŸ“ ä¿å­˜æ•°æ®: {filepath}")
    return str(filepath)

def start_event_recording(frame):
    """äº‹ä»¶è§†é¢‘"""
    global event_writer, event_start_time
    
    if event_writer:
        event_writer.release()
    
    timestamp = get_timestamp()
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = 25
    size = frame.shape[1], frame.shape[0]
    
    filename = f"event_{len(list(EVENTS_DIR.glob('*.mp4'))):04d}_{timestamp}.mp4"
    event_path = EVENTS_DIR / filename
    
    event_writer = cv2.VideoWriter(str(event_path), fourcc, fps, size)
    event_start_time = time.time()
    logger.warning(f"ğŸš¨ äº‹ä»¶å½•åˆ¶: {event_path}")
    return event_path

def process_frame(frame, model, frame_id):
    """æ ¸å¿ƒæ£€æµ‹+è·Ÿè¸ªé€»è¾‘"""
    # ä½¿ç”¨å†…ç½®è·Ÿè¸ªåŠŸèƒ½
    results = model.track(
        source=frame,
        persist=True,  # ä¿æŒTrack ID
        conf=conf_threshold,
        classes=classes_to_detect,
        verbose=False,
        tracker="botsort.yaml",  # å†…ç½®BOTSORT
        project="temp",  # ä¸´æ—¶é¡¹ç›®ï¼Œé¿å…ä¿å­˜
        name="temp"      # ä¸´æ—¶åç§°
    )
    
    detections = []
    result = results[0]  # å•å¸§ç»“æœ
    
    if result.boxes is not None:
        boxes = result.boxes.xyxy.cpu().numpy()
        confidences = result.boxes.conf.cpu().numpy()
        class_ids = result.boxes.cls.cpu().numpy()
        track_ids = result.boxes.id
        
        # å¤„ç†è·Ÿè¸ªID
        if track_ids is not None:
            track_ids = track_ids.int().cpu().numpy()
        else:
            track_ids = np.arange(len(boxes))  # å¤‡ç”¨ID
            
        for i in range(len(boxes)):
            box = boxes[i]
            conf = confidences[i]
            cls_id = class_ids[i]
            track_id = int(track_ids[i])
            
            x1, y1, x2, y2 = map(int, box)
            
            # åˆ†ç±»å’Œè¡Œä¸º
            obj_type = classify_object(int(cls_id))
            behavior = analyze_behavior([x1, y1, x2, y2], frame.shape)
            
            detection = {
                "track_id": track_id,
                "type": obj_type,
                "class_name": model.names[int(cls_id)],
                "confidence": float(conf),
                "bbox": [x1, y1, x2, y2],
                "behavior": behavior
            }
            detections.append(detection)
    
    return frame, detections

def save_worker():
    """å¼‚æ­¥ä¿å­˜"""
    from queue import Empty

    while True:
        try:
            task = save_queue.get(timeout=1)
            if task is None:
                break
            frame_id, frame, detections = task
            image_path = save_detection_image(frame.copy(), detections, frame_id)
            if image_path:
                save_detection_data(detections, image_path, frame_id)
            save_queue.task_done()
        except Empty:
            # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
            continue
        except Exception as e:
            logger.error(f"ä¿å­˜é”™è¯¯: {e}", exc_info=True)

# ä¸»ç¨‹åºåˆå§‹åŒ–
cap = cv2.VideoCapture(RTSP_URL)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

if not cap.isOpened():
    logger.error("æ— æ³•æ‰“å¼€RTSPæµ")
    exit(1)

# å¯åŠ¨ä¿å­˜çº¿ç¨‹
save_thread = threading.Thread(target=save_worker, daemon=True)
save_thread.start()

# é¢„çƒ­
logger.info("é¢„çƒ­æ¨¡å‹...")
for i in range(10):
    ret, frame = cap.read()
    if ret:
        frame, _ = process_frame(frame, model, i)

logger.info("ğŸš€ å¼€å§‹ç›‘æ§...")
prev_time = time.time()
frame_count = 0

try:
    while True:
        ret, frame = cap.read()
        if not ret:
            logger.warning("RTSPé‡è¿...")
            cap.release()
            cap = cv2.VideoCapture(RTSP_URL)
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            continue
        
        frame_id = frame_count
        annotated_frame, detections = process_frame(frame, model, frame_id)
        
        # è¾“å‡ºå’Œä¿å­˜
        if detections:
            console_output(detections, frame_id)
            if not save_queue.full():
                save_queue.put((frame_id, annotated_frame.copy(), detections))
        
        # äº‹ä»¶å½•åˆ¶
        if SAVE_EVENTS and detections:
            if event_writer is None:
                start_event_recording(annotated_frame)
            event_writer.write(annotated_frame)
            if time.time() - event_start_time > EVENT_DURATION:
                event_writer.release()
                event_writer = None
        elif event_writer:
            event_writer.release()
            event_writer = None
        
        # FPSæ˜¾ç¤º
        frame_count += 1
        if frame_count % 30 == 0:
            curr_time = time.time()
            fps = 30 / (curr_time - prev_time)
            prev_time = curr_time
            cv2.putText(annotated_frame, f'FPS:{fps:.1f}', (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv2.imshow('æ™ºèƒ½ç›‘æ§', annotated_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    logger.info("ç”¨æˆ·ä¸­æ–­")

finally:
    save_queue.put(None)
    save_thread.join(timeout=5)
    if event_writer:
        event_writer.release()
    cap.release()
    cv2.destroyAllWindows()
    
    logger.info(f"ğŸ“ˆ ç»Ÿè®¡: å¸§={frame_count}, è½¨è¿¹={len(unique_tracks)}, "
                f"å›¾åƒ={len(list(IMAGES_DIR.glob('*.jpg')))}")